Estimating the 6D pose of objects in hand is a challenging and critical problem in computer vision and robotics, particularly for applications like robotic manipulation, human-robot interaction, and augmented reality (AR). Leveraging multi-modal data, such as color (RGB) and depth (D) images, offers a promising approach. Despite significant progress in representation learning across these modalities, this field faces two major issues. First, the presence of a hand often causes significant occlusions, which conventional object pose estimation methods struggle to handle effectively. Second, existing approaches typically extract features from two separate backbones and fuse the data at the feature level. This fusion mechanism can cause a representation distribution shift and potential disruption during fine-tuning due to dense interactions between the RGB and depth branches. In this work, we propose a novel deep neural network for hand-held object pose estimation using an RGB-D image as input. Instead of fusing data at the feature level, we introduce a vote-based fusion mechanism to effectively combine multimodal data for the pose estimation task, particularly for occluded objects in the presence of a hand. Additionally, we model the relationship between the hand and the object through keypoint interaction, incorporating this relationship into the object pose estimation process. Through experiments on three public datasets, our approach demonstrates significant improvements in accuracy and robustness over existing methods, achieving an accuracy improvement of up to 15\%.
