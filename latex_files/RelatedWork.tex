\section{Related work}
\label{sec:relatedwork}

\subsection{Object Pose Estimation From Visual Inputs}

Object pose estimation is a well-explored field in computer vision, focusing on determining the position and orientation (pose) of objects from visual inputs such as RGB or depth images \cite{wang2019densefusion, he2020pvn3d, he2021ffb6d}. Deep learning-based methods have significantly advanced pose estimation by learning rich feature representations directly from the data \cite{wang2021gdr, peng2019pvnet}. These methods can be broadly categorized into two main types: depth-based methods, RGB-based methods, RGBD-based methods. Depth-based object pose estimation uses depth sensors to capture the 3D structure of objects, extracting geometric information from depth maps \cite{wang20216d, gao20206d, guo2021efficient}. This approach is beneficial in low-light conditions or when RGB-based methods fail due to insufficient texture. However, depth sensors can produce noisy or incomplete data, especially with occlusions or reflective surfaces, and processing dense point clouds can be computationally expensive for real-time applications. RGB-based methods estimate object poses using color images, leveraging texture and color information to identify and localize objects \cite{billings2019silhonet, peng2019pvnet, wang2021gdr}. Common techniques include keypoint detection, feature matching, and deep learning-based regression. These methods perform well in well-lit environments with distinct textures but struggle with varying lighting conditions and occlusions. They are also less effective for objects with uniform or repetitive textures, where the lack of distinct features makes pose estimation challenging. 

RGBD-based methods combine the strengths of both RGB and depth-based approaches by leveraging multimodal data \cite{wang2019densefusion, he2020pvn3d, hong2024rdpn6d}. These methods utilize both color and depth information to provide a more comprehensive understanding of the object's geometry and appearance. By integrating RGB and depth data, RGBD-based methods can achieve higher accuracy and robustness in pose estimation. Existing data fusion methods often involve combining features from RGB and depth modalities at various stages of the neural network. Common approaches include early fusion (combining raw data), middle fusion (combining intermediate features), and late fusion (combining final predictions). Each method has its advantages and drawbacks. Early fusion can lead to high-dimensional inputs and increased computational costs, while late fusion may miss out on capturing interactions between modalities during feature extraction. Middle fusion strikes a balance but can still suffer from representation distribution shifts. In hand-held object pose estimation, the presence of the hand introduces non-rigid transformations and occlusions that further complicate the fusion process. Existing methods may not effectively model the interaction between the hand and the object, leading to reduced accuracy in pose estimation. In this work, we propose a novel deep neural network for hand-held object pose estimation using an RGB-D image as input. Instead of fusing data at the feature level, we introduce a vote-based fusion mechanism to effectively combine multimodal data for the pose estimation task, particularly for occluded objects in the presence of a hand. 

\subsection{Voting Mechanism In Visual Tasks}

Voting mechanisms have been widely used in computer vision tasks to enhance robustness and accuracy. These mechanisms involve aggregating multiple hypotheses or predictions to arrive at a consensus, which is particularly useful in noisy or ambiguous scenarios. The use of voting in visual tasks can be traced back to classical techniques like the Hough Transform, which aggregates votes in a parameter space to detect shapes in images \cite{hough1959machine}. In recent years, voting mechanisms have been integrated into deep learning frameworks to improve tasks such as object detection or pose estimation. VoteNet \cite{qi2019deep}, for instance, employs a voting scheme where 3D points cast votes for object centers, leading to robust object detection in point clouds. Inspired by this, Xie et al. \cite{xie2021venet} developed an enhanced VoteNet-based detector for cluttered indoor scenes, replacing the traditional MLP with an Attentive MLP (AMLP) for better feature description. Further, Mlcvnet \cite{xie2020mlcvnet} introduced Multi-Level Context VoteNet (MLCVNet), integrating context modules into voting and classification to encode contextual information at multiple levels. These include the Patch-to-Patch Context (PPC) for point patches, Object-to-Object Context (OOC) for object candidates, and Global Scene Context (GSC) for the overall scene. In RGB object pose estimation, voting mechanisms have proven highly effective. Peng et al. \cite{peng2019pvnet} introduced PVNet, which uses pixel-wise vector regression to vote for keypoint locations, aiding in the localization of occluded or truncated keypoints. He et al. \cite{he2020pvn3d} extended this by detecting 3D keypoints and estimating 6D pose parameters through a deep Hough voting network. Di et al. \cite{di2022gpv} later proposed GPV-Pose, which leverages geometric constraints for category-level pose estimation, using a 3D graph convolution encoder with branches for pose regression, symmetry-aware reconstruction, and point-wise bounding box voting.

Inspired by these successful implementations of voting mechanisms, we introduce a novel vote-based fusion mechanism for RGB-D images. Unlike existing methods that generate votes from depth images, RGB images, or fused RGB-D features, our approach separately performs voting in the 2D image branch and the 3D point cloud branch before combining the results. By leveraging both 2D and 3D keypoints, this method integrates multimodal data more effectively than traditional feature-level fusion methods, significantly enhancing pose estimation accuracy, especially in scenarios involving occluded objects.
