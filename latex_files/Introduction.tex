\section{Introduction}
\label{sec:intro}

Estimating the 6D pose of hand-held objects is an important yet challenging task in robotics (\cite{pfanne2018fusing, anzai2020deep, okafor2024deep}), virtual reality \cite{muhanna2015virtual}, among others. It plays a significant role in applications such as robotic manipulation or human-robot interaction (\cite{andrychowicz2020learning, handa2020dexpilot}). The increasing availability of 3D sensors has made RGB-D data more accessible, which, when incorporating 3D geometric information from depth sensors, allows for more accurate and robust 6D pose estimation compared to using RGB data alone. However, despite notable advancements in multi-modal representation learning, significant challenges remain. These challenges are primarily due to occlusions caused by the hand and the complexities involved in effectively fusing RGB and depth data (\cite{chao2021dexycb}). Traditional methods (\cite{wang20216d, peng2019pvnet, wang2019densefusion, sharma2022saliency}) struggle with accuracy drops when hands occlude objects, as hands obscure critical features needed for precise pose estimation. These occlusions introduce ambiguities and complexities, making it difficult to determine the object's orientation and position. Some approaches attempt to segment the hand or predict occluded regions, but these add complexity and often fall short under challenging conditions (\cite{he2020pvn3d, castro2023crt}). Additionally, hand-object interactions introduce non-rigid transformations, further complicating pose estimation by distorting the object's perceived shape and pose due to varying grips and manipulations.

Current mainstream RGBD fusion methods typically employ separate RGB pre-trained backbones to extract features from RGB images and depth maps (\cite{wang2019densefusion, he2020pvn3d, he2021ffb6d}). These backbones independently extract features from their respective modalities, which are then integrated during subsequent processing stages. Despite achieving high performance on benchmark datasets, several critical issues persist. The RGBD backbones are designed to handle input as image-depth pairs, which contrasts with the single-image input used in RGB pretraining. This difference often leads to a significant shift in representation distribution. Specifically, the features learned from single RGB images may not directly align with those extracted from RGBD inputs, impacting the model's ability to generalize effectively across both modalities. Moreover, during fine-tuning, extensive interactions occur between the RGB and depth branches. These interactions are crucial for integrating complementary information from both sources. However, the dense coupling between these branches can potentially disturb the original representation distribution learned by the pretrained RGB backbone. This phenomenon may hinder the model's capacity to leverage the full potential of the RGB features, thereby affecting overall performance and generalization ability on RGBD tasks.

In this paper, we present a novel deep neural network specifically designed for hand-held object pose estimation using RGB-D images. Different from traditional feature-level fusion methods, our approach introduces a vote-based fusion mechanism that effectively integrates multimodal data. This mechanism leverages a voting scheme where both 2D and 3D keypoints cast votes for the object's pose, particularly enhancing the estimation in scenarios with occluded objects. By combining votes from different modalities, our method mitigates the issues of representation distribution shift and potential disruptions during fine-tuning, leading to more accurate pose predictions. Furthermore, we model the interaction between the hand and the object through self-attention mechanism, integrating this relationship into the pose estimation process. Our network predicts keypoints for both the hand and the object, using these keypoints to establish a contextual relationship that enhances the pose estimation, especially when the object is partially obscured by the hand. This hand-object keypoint interaction modeling allows our method to account for non-rigid transformations and occlusions introduced by the hand, significantly improving the robustness of the pose estimation.

The primary contributions of this work can be summarized as follows:

\begin{itemize}

\item \textbf{Vote-based Fusion Module} $\mathcal{M}_{fus}$: We introduce a vote-based fusion mechanism that integrates RGB and depth data more effectively than traditional feature-level fusion methods. This module utilizes both 2D and 3D features, leveraging a radius-based neighborhood projection and channel attention mechanisms to enhance the representation of crucial features while mitigating noise and redundancy. \\

\item \textbf{Hand-aware Object Pose Estimation Module} $\mathcal{M}_{hao}$: By modeling the relationship between the hand and the object through a self-attention mechanism, we enhance the pose estimation process. This self-attention mechanism captures complex spatial relationships between keypoints, which is crucial for accurately estimating the pose of objects that are partially occluded by the hand.

\end{itemize}